{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"JpGeQAYbLXpl","executionInfo":{"status":"ok","timestamp":1746695744211,"user_tz":420,"elapsed":21618,"user":{"displayName":"Justin Anderson","userId":"12208572201846891092"}},"outputId":"56632690-930b-4a6f-8731-86c251fbb11b","colab":{"base_uri":"https://localhost:8080/"}},"id":"JpGeQAYbLXpl","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"ikfHjMj6LTaY"},"source":["# Step 2: Preprocessing Transportation Documents\n","This notebook loads raw files from `/data/raw/`, cleans and chunks them for embedding or fine-tuning."],"id":"ikfHjMj6LTaY"},{"cell_type":"code","metadata":{"id":"6r_gVZTgLTaZ"},"source":["import os\n","import re\n","import glob\n","from pathlib import Path\n","\n","RAW_DIR = '../data/raw'\n","CLEAN_DIR = '../data/cleaned'\n","os.makedirs(CLEAN_DIR, exist_ok=True)\n"],"id":"6r_gVZTgLTaZ","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fpZuDQdnLTaa"},"source":["## Load and Clean Text Files"],"id":"fpZuDQdnLTaa"},{"cell_type":"code","metadata":{"id":"3-OveLBILTaa"},"source":["def clean_text(text):\n","    text = re.sub(r'\\s+', ' ', text)\n","    text = re.sub(r'\\n+', '\\n', text)\n","    return text.strip()\n","\n","for file_path in glob.glob(f'{RAW_DIR}/*.txt'):\n","    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n","        raw_text = f.read()\n","    cleaned = clean_text(raw_text)\n","    fname = Path(file_path).stem + '_cleaned.txt'\n","    with open(os.path.join(CLEAN_DIR, fname), 'w', encoding='utf-8') as out:\n","        out.write(cleaned)\n","    print(f'Cleaned: {fname}')"],"id":"3-OveLBILTaa","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tCRpmXVVLTab"},"source":["## Optional: Split Text into Chunks"],"id":"tCRpmXVVLTab"},{"cell_type":"code","metadata":{"id":"N5I4MGo_LTab"},"source":["CHUNK_SIZE = 1000  # characters\n","OVERLAP = 200\n","\n","def split_text(text, size=CHUNK_SIZE, overlap=OVERLAP):\n","    return [text[i:i+size] for i in range(0, len(text), size - overlap)]\n","\n","for file_path in glob.glob(f'{CLEAN_DIR}/*_cleaned.txt'):\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        text = f.read()\n","    chunks = split_text(text)\n","    for i, chunk in enumerate(chunks):\n","        out_path = file_path.replace('_cleaned.txt', f'_chunk_{i}.txt')\n","        with open(out_path, 'w', encoding='utf-8') as out:\n","            out.write(chunk)\n","    print(f'Chunked: {file_path}')"],"id":"N5I4MGo_LTab","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}